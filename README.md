# Cheminformatics_ML_Comparison

Code for "A Brief Survey of ML Methods Predicting Molecular Solubility: Towards Lighter Models via Attention and Hyperparameter Optimization"

Reference: Lew, A. A Brief Survey of ML Methods Predicting Molecular Solubility: Towards Lighter Models via Attention and Hyperparameter Optimization. Preprints 2024, 2024090849. https://doi.org/10.20944/preprints202409.0849.v1 

A comparative illustration on predicting molecular properties from chemical structure using RDKit, scikit-learn, and PyTorch - from simple Linear Regression to Bayesian Optimized Transformer Encoder Networks. 

Here I use the MoleculeNet ESOL dataset (https://moleculenet.org/datasets-1), which provides water solubility data (log solubility in mols/L) for common organic small molecules. First, various molecular representations (RDKit fingerprint, Morgan fingerprint, and a vector of selected molecular features) are used to train a series of regression models (linear, ridge, lasso, and multilayer perceptron). Next, convolution and attention-based graph neural networks are used on a node and edge representation of the atoms and bonds, respectively, to address drawbacks with the previous methodologies. Finally, I adapt a transformer encoder layer to predict solubilities directly from SMILES strings in an approach analogous to linguistic sentiment analysis - treating SMILES as "linguistic passages" and solubilities as the "sentiment".

As expected, the simplest linear regression is poorly performant at predicting properties, while the lasso regression which incorporates regularization penalizing the absolute value magnitude of coefficients performs better, and the deep learning multilayer perception approach performs even better. For these regressors, using a limited vector of selected chemical features that directly represent physical attributes provides better results than the more complicated fingerprints that fully encode each molecule, as we have already pre-selected key information from each molecule. The drawbacks of these approaches however are that 1) a priori selection of features is required for best performance - which is not always possible, especially in the most interesting regimes of novel discovery and 2) the deep learning multilayer perceptron requires training many parameters, in this case ~70k.

To mitigate these issues, I next use graph representations of each molecule, where atoms are nodes and bonds are undirected edges, and use a graph convolutional network to perform graph regression for property predicton. In this way, 1) I do not need to encode a priori selected features that are relevant to the prediction task and 2) am able to achieve comparable performance to the multilayer perceptron with a fraction of the parameters, in this case ~13k or <20% of the multilayer perceptron. Next, I employ a multiheaded graph attention network to allow the model to learn how much to weight the contribution of neighboring nodes to each atom's embedded representation. While the resultant test performance is slightly worse than the previous graph convolution network (MSE 1.1 vs 0.8, r-squared 0.76 vs 0.83) we again use only a fraction of the parameters, 829 or <7% of the graph convolutional network.

Finally, I apply an attention encoder network directly on the SMILES strings to obtain highly performant results without requiring either pre-selected features or complicated node edge molecular representations. The attention mechanism allows for the model to learn relatonships between non-adjacent tokens, crucial for tracking the impacts of atoms at different spatial locations throughout each molecule on solubility. Ultimately, I implement Bayesian Optimization to tune the hyperparameters of the attention encoder model. To expedite exploration of hyperparameter space, for the objective function I restrict training to only 200 epochs instead of the 3000 epochs originally used. After identifying the optimially performing set of hyperparameters, I then train a model with the full 3000 epochs to obtain even better performance (MSE 0.71, r-squared 0.85) than the original attention encoder model using less than 1/3 of the parameters (2717 vs 8513)
